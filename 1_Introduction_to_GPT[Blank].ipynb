{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/iggyloh/Coursera/blob/main/1_Introduction_to_GPT%5BBlank%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9513b82c",
      "metadata": {
        "id": "9513b82c"
      },
      "source": [
        "# Introduction to GrabGPT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bd7ba4a",
      "metadata": {
        "id": "2bd7ba4a"
      },
      "source": [
        "## Learning Objectives\n",
        "1. Setting up the required variables to call the endpoint GrabGPT API\n",
        "2. Making chat compltion calls to library\n",
        "3. Handling intermittent network and rate limit issues gracefully"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "80164b95",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80164b95",
        "outputId": "b619649b-f596-4cab-e2b2-9066653fab41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.2.3)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (0.3.12)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.4)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.52.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.8.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (0.1.136)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (24.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (2.9.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (9.0.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core) (4.12.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (3.10.9)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.125->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (0.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.52.0->langchain-openai) (4.66.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.52.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain-core) (0.14.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install python-dotenv langchain-openai langchain-core langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0fee8994-ca19-4fa0-b586-c8b94f4af0de",
      "metadata": {
        "tags": [],
        "id": "0fee8994-ca19-4fa0-b586-c8b94f4af0de"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "03fa41e2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03fa41e2",
        "outputId": "57e39034-50df-4a7d-bac0-3a1462e9f615"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Ensure environment variables are loaded before accessing them\n",
        "load_dotenv('template.env')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1acbce11",
      "metadata": {
        "id": "1acbce11"
      },
      "source": [
        "### Set the API Key environment\n",
        "- gpt-4\n",
        "- gpt-3.5-turbo\n",
        "- langsmith\n",
        "- Additional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3db16bb9",
      "metadata": {
        "id": "3db16bb9"
      },
      "outputs": [],
      "source": [
        "# GPT 4\n",
        "open_api_key = os.getenv('OPENAI_API_KEY')\n",
        "gpt = ChatOpenAI(model='gpt-4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddf6cd24",
      "metadata": {
        "id": "ddf6cd24"
      },
      "outputs": [],
      "source": [
        "# GPT 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XQJCO-KSMgeG"
      },
      "outputs": [],
      "source": [
        "# GPT 3\n",
        "open_api_key = os.getenv('OPENAI_API_KEY')\n",
        "gpt = ChatOpenAI(model='gpt-3.5-turbo')"
      ],
      "id": "XQJCO-KSMgeG"
    },
    {
      "cell_type": "markdown",
      "id": "4b4ce8a9",
      "metadata": {
        "id": "4b4ce8a9"
      },
      "source": [
        "### Introduction to LangSmith\n",
        "\n",
        "LangSmith is a tool in the Langchain ecosystem designed to help monitor, evaluate, and debug language models more effectively. It provides the infrastructure for logging interactions, running tests, and tracking metrics to ensure optimal model performance.\n",
        "\n",
        "We will be covering this topic in depth in future sessions!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "e187e4e1",
      "metadata": {
        "id": "e187e4e1"
      },
      "outputs": [],
      "source": [
        "# Last step: LangSmith\n",
        "tracing = os.getenv('LANGCHAIN_TRACING_V2')\n",
        "langsmith = os.getenv('LANGCHAIN_API_KEY')\n",
        "# Let's you trace everything that is going on in this codespace."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0847e38",
      "metadata": {
        "id": "d0847e38"
      },
      "source": [
        "# What is GPT?\n",
        "\n",
        "GPT refers to a suite of powerful language models developed by OpenAI, such as **GPT-3**, **GPT-4**, and **GPT-4 with Vision**. These models can perform a wide variety of tasks, including text generation, conversation, translation, and image processing (in the case of GPT-4 with Vision).\n",
        "\n",
        "Through OpenAI's API, users can access these models directly via OpenAI’s platform, bypassing the need for third-party integrations like Azure. With an **OpenAI API key**, you have direct access to the latest models OpenAI offers.\n",
        "\n",
        "While our focus for this training is on OpenAI’s models, which are the most widely used and well-documented, the principles for interacting with other large language models (LLMs) like Meta’s LLaMA or Anthropic’s Claude are largely similar.\n",
        "\n",
        "## Available Models through OpenAI API:\n",
        "\n",
        "| Model Name                      | Description                               |\n",
        "|----------------------------------|-------------------------------------------|\n",
        "| **gpt-3.5-turbo**                | A highly efficient variant of GPT-3.5, great for most conversational tasks. |\n",
        "| **gpt-4**                        | The latest iteration of OpenAI’s powerful GPT series, offering enhanced understanding and reasoning abilities. |\n",
        "| **gpt-4-32k**                    | A larger version of GPT-4 with the ability to process longer context windows. |\n",
        "| **gpt-4 with Vision**            | Allows GPT-4 to process both text and images, useful for tasks that combine visual and textual inputs. |\n",
        "| **text-embedding-ada-002**       | A model specialized for creating embeddings for tasks like text similarity, search, and clustering. |\n",
        "| **Whisper**                      | OpenAI’s speech recognition model for transcribing and translating audio. |\n",
        "| **DALL·E**                       | OpenAI’s image generation model, capable of generating images from textual descriptions. |\n",
        "\n",
        "For a comprehensive list of models and capabilities available through OpenAI’s API, you can refer to the [OpenAI API documentation](https://platform.openai.com/docs).\n",
        "\n",
        "---\n",
        "\n",
        "This version reflects the use of the OpenAI API key for direct access to OpenAI's models, rather than relying on Azure or other providers. Let me know if you'd like further adjustments!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "69a4ed35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69a4ed35",
        "outputId": "45a00cc5-d5cd-4329-a341-804199ab5453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='我在星巴克遇见了一位社交媒体影响者' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 25, 'total_tokens': 48, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-46f7b09a-9e05-440b-86b2-3a6ad2e0c0fb-0' usage_metadata={'input_tokens': 25, 'output_tokens': 23, 'total_tokens': 48, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n",
            "我在星巴克遇见了一位社交媒体影响者\n"
          ]
        }
      ],
      "source": [
        "# gpt 4\n",
        "# Run once can le!\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "messages = [\n",
        "    SystemMessage(content = 'Translate the following from English into Chinese'),\n",
        "    HumanMessage(content = 'I met an influencer at Starbucks')\n",
        "]\n",
        "\n",
        "message = gpt.invoke(messages)\n",
        "print(message)\n",
        "print(message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26dc383b",
      "metadata": {
        "id": "26dc383b"
      },
      "source": [
        "### Tokens\n",
        "The token limit determines how much information can be handled in a single interaction. For large documents or complex tasks, a higher token limit allows for more extensive input and output, while lower limits mean shorter interactions.\n",
        "\n",
        "Why is this important?\n",
        "The token limit determines how much information can be handled in a single interaction. For large documents or complex tasks, a higher token limit allows for more extensive input and output, while lower limits mean shorter interactions.\n",
        "\n",
        "GPT-4 (8k context model): This version has a maximum token limit of 8,192 tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "401f78dd",
      "metadata": {
        "id": "401f78dd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca539ff",
      "metadata": {
        "id": "4ca539ff"
      },
      "outputs": [],
      "source": [
        "# gpt 3.5 turbo\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1643b88d",
      "metadata": {
        "id": "1643b88d"
      },
      "source": [
        "# Try it out yourself!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "11873c8c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11873c8c",
        "outputId": "47f9992f-b470-4294-b654-df0499e5b341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A traditional Singaporean breakfast is usually a hearty and satisfying meal that reflects the country's diverse cultural influences. Some popular breakfast options in Singapore include:\n",
            "\n",
            "1. Kaya toast: A quintessential Singaporean breakfast item, kaya toast consists of toasted bread spread with kaya (a sweet coconut and egg jam) and a slice of butter. It is often served with soft-boiled eggs and a cup of coffee or tea.\n",
            "\n",
            "2. Roti prata: A South Indian-influenced dish, roti prata is a type of crispy flatbread that is usually served with a variety of curries or dhal for dipping. It is a popular breakfast option at local hawker centers.\n",
            "\n",
            "3. Nasi lemak: A Malay dish that is enjoyed throughout the day, nasi lemak is a fragrant coconut rice dish served with fried chicken, anchovies, peanuts, boiled egg, and sambal chili. It is a flavorful and filling breakfast choice.\n",
            "\n",
            "4. Bak chor mee: A Teochew-style dish, bak chor mee is a noodle dish typically served with minced pork, mushrooms, and vinegar. It is a savory and satisfying breakfast option that can be found at many hawker stalls.\n",
            "\n",
            "5. Dim sum: While not traditionally Singaporean, dim sum is a popular breakfast choice in the country due to its influences from Cantonese cuisine. Dim sum restaurants offer a wide variety of bite-sized dishes such as steamed dumplings, buns, and rolls.\n",
            "\n",
            "These are just a few examples of the diverse breakfast options you can find in Singapore. The country's vibrant food scene offers something for every palate, making breakfast a delicious and enjoyable meal to start the day.\n",
            "{'token_usage': {'completion_tokens': 342, 'prompt_tokens': 27, 'total_tokens': 369, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "1) Ask chatgpt for Singapore's most popular breakfast meal!\n",
        "2) Take note of the tokens used\n",
        "3) Find out the cost!\n",
        "'''\n",
        "messages = [\n",
        "    SystemMessage(content='You are a Singaporean foodie expert'),\n",
        "    HumanMessage(content = 'What is a Singaporean breakfast like?'),\n",
        "]\n",
        "output = gpt.invoke(messages)\n",
        "print(output.content)\n",
        "print(output.response_metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4369aa80",
      "metadata": {
        "id": "4369aa80"
      },
      "outputs": [],
      "source": [
        "# Content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ab72657",
      "metadata": {
        "id": "7ab72657"
      },
      "outputs": [],
      "source": [
        "# Token usage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "253b232f",
      "metadata": {
        "id": "253b232f"
      },
      "source": [
        "## Prompt Engineering & Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "d9b44198",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9b44198",
        "outputId": "c5fe3012-b673-42d6-d6ea-7b56d4ed45b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-----Prompt from Template-----\n",
            "messages=[HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})]\n",
            "Why was the cat sitting on the computer?\n",
            "\n",
            "Because it wanted to keep an eye on the mouse!\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate # Mimic what you will see when using ChatGPT UI\n",
        "\n",
        "# PART 1: Create a ChatPromptTemplate using a template string\n",
        "print(\"-----Prompt from Template-----\")\n",
        "template = 'Tell me a joke about {topic}'\n",
        "prompt_template = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "prompt = prompt_template.invoke({'topic':'cats'})\n",
        "print(prompt)\n",
        "\n",
        "result = gpt.invoke(prompt)\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d364f17",
      "metadata": {
        "id": "2d364f17"
      },
      "source": [
        "#### You will notice there is alot of room for dynamic changes using a prompt template as compared to using the standard chat template above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15d6310d",
      "metadata": {
        "id": "15d6310d"
      },
      "outputs": [],
      "source": [
        "# PART 2: Prompt with Multiple Placeholders\n",
        "print(\"\\n----- Prompt with Multiple Placeholders -----\\n\")\n",
        "template_multiple = \"\"\"You are a helpful assistant.\n",
        "Human: Tell me a {adjective} short story about a {animal}.\n",
        "Assistant:\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "91a8b020",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91a8b020",
        "outputId": "f8f94c22-64a1-4704-bdac-cfbdfcb43537"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "----- Prompt with System and Human Messages (Tuple) -----\n",
            "\n",
            "1. Why did the lawyer bring a ladder to the courtroom? Because he heard the case was going to be heard on a higher level!\n",
            "\n",
            "2. What do you call a group of lawyers at the bottom of the ocean? A good start!\n",
            "\n",
            "3. How many lawyers does it take to change a lightbulb? None, they'd rather keep you in the dark and bill you for it!\n"
          ]
        }
      ],
      "source": [
        "# PART 3: Prompt with System and Human Messages (Using Tuples)\n",
        "print(\"\\n----- Prompt with System and Human Messages (Tuple) -----\\n\")\n",
        "\n",
        "messages = [\n",
        "    (\"system\", \"You are a comedian who tells jokes about {topic}.\"),\n",
        "    (\"human\", \"Tell me {joke_count} jokes.\"),\n",
        "]\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "prompt = prompt_template.invoke({'topic':'lawyers', 'joke_count':'3'})\n",
        "result = gpt.invoke(prompt)\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d938831e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d938831e",
        "outputId": "8447cb92-fdfe-4fc2-b618-a35b8d2b1371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, I'd be happy to help you with that calculus problem!\n",
            "\n",
            "To solve the integral of (3x^2 - 2x + 1) with respect to x from 0 to 2, we will first find the antiderivative of the function.\n",
            "\n",
            "The antiderivative of 3x^2 with respect to x is x^3, the antiderivative of -2x with respect to x is -x^2, and the antiderivative of 1 with respect to x is x.\n",
            "\n",
            "Therefore, the antiderivative of the given function is x^3 - x^2 + x.\n",
            "\n",
            "Next, we will evaluate this antiderivative at the upper and lower limits of integration:\n",
            "\n",
            "At x = 2: (2)^3 - (2)^2 + 2 = 8 - 4 + 2 = 6\n",
            "At x = 0: (0)^3 - (0)^2 + 0 = 0 - 0 + 0 = 0\n",
            "\n",
            "Finally, we will subtract the value of the antiderivative at the lower limit from the value at the upper limit:\n",
            "\n",
            "6 - 0 = 6\n",
            "\n",
            "Therefore, the value of the definite integral of (3x^2 - 2x + 1) from 0 to 2 is 6.\n"
          ]
        }
      ],
      "source": [
        "# Try it out yourself\n",
        "\"\"\"\n",
        "Write a Python code snippet using ChatPromptTemplate to:\n",
        "\n",
        "1) Create the system and human messages using tuples.\n",
        "2) The system message should say: \"You are a calculus expert tutor.\"\n",
        "3) The human message should say: \"Help me solve {problem_count} calculus problems.\"\n",
        "4) Use problem_count = \"\\int_0^2 (3x^2 - 2x + 1) \\, dx\" as an input.\n",
        "5) Invoke the prompt and print the response from the model.\n",
        "\"\"\"\n",
        "messages = [\n",
        "    'system', 'You are a calculus expert tutor',\n",
        "    'human', 'Help me solve {problem_count} calculus problems',\n",
        "]\n",
        "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
        "prompt = prompt_template.invoke({'problem_count': \"\\int_0^2 (3x^2 - 2x + 1) \\, dx\"})\n",
        "result = gpt.invoke(prompt)\n",
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8133dfbd",
      "metadata": {
        "id": "8133dfbd"
      },
      "source": [
        "### Additional\n",
        "\n",
        "From lazy prompt to detailed prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e05917b0",
      "metadata": {
        "id": "e05917b0"
      },
      "outputs": [],
      "source": [
        "# We can take prompts that were pre made by people!\n",
        "from langchain import hub\n",
        "prompt = hub.pull(\"hardkothari/prompt-maker\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ec3c9dc",
      "metadata": {
        "id": "8ec3c9dc"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(new_prompt.messages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02f625f7",
      "metadata": {
        "id": "02f625f7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc90f635",
      "metadata": {
        "id": "dc90f635"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a0d55772",
      "metadata": {
        "id": "a0d55772"
      },
      "source": [
        "## Few-Shot Prompt Template Example\n",
        "In this section, we'll learn how to create a simple prompt template that helps guide the model by providing example inputs and outputs. This technique, known as few-shotting, involves showing the model a few examples to help it understand the task better. It is a powerful way to improve the quality of the generated output, especially when the task is complex or context-dependent.\n",
        "\n",
        "A few-shot prompt template can be built from a fixed set of examples or can be dynamically constructed using an Example Selector class, which is responsible for selecting relevant examples from a pre-defined set based on the query.\n",
        "\n",
        "## Parameter Explanations\n",
        "\n",
        "#### Prompt Template: A framework that structures your prompt and integrates a set of few-shot examples to guide the model's behavior.\n",
        "\n",
        "#### Few-Shotting: Providing a series of example inputs and outputs to the model in the prompt. This helps the model generate better responses by mimicking the patterns in the examples.\n",
        "\n",
        "If you want to read more about [Few-Shot-Prompting Papers](https://arxiv.org/abs/2005.14165)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9a49079",
      "metadata": {
        "id": "a9a49079"
      },
      "source": [
        "## Difference Between Zero-Shot, One-Shot, and Few-Shot Learning\n",
        "\n",
        "- **Zero-Shot Learning**: In zero-shot learning, the model is able to perform a task without having seen any examples or prior data for that specific task. It relies on knowledge transfer from other tasks or context provided by the model.\n",
        "\n",
        "- **One-Shot Learning**: In one-shot learning, the model is trained on only one example of each task or class and is expected to generalize well enough to perform accurately on new, unseen data.\n",
        "\n",
        "- **Few-Shot Learning**: In few-shot learning, the model is trained on a small number of examples (usually a handful) for each class or task, and it uses this limited data to make predictions or perform the task on new examples.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88bc582d",
      "metadata": {
        "id": "88bc582d"
      },
      "source": [
        "# Zero-shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fbce165",
      "metadata": {
        "id": "0fbce165"
      },
      "outputs": [],
      "source": [
        "# Zero-shot example: Sentiment analysis (classification task with no prior examples)\n",
        "\n",
        "# System message defines the assistant's role\n",
        "content=\"You are a helpful assistant. Who is great at picking up the nuances in a sentence and direct in the way you respond\"\n",
        "\n",
        "# Human message asks the model to classify the sentiment of the sentence\n",
        "content=\"Classify the following sentence as positive, negative, or neutral: 'The product quality is excellent and I love it!'\"\n",
        "\n",
        "# Send the conversation to the model\n",
        "\n",
        "\n",
        "# Output the model's response\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6c81a9d",
      "metadata": {
        "id": "f6c81a9d"
      },
      "source": [
        "## This can be problematic when the task is very ambiguous"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3740ad48",
      "metadata": {
        "id": "3740ad48"
      },
      "outputs": [],
      "source": [
        "content=\"You are a helpful assistant. Who is great at picking up the nuances in a sentence and direct in the way you respond.\"\n",
        "\n",
        "content= \"Classify the following sentence as positive, negative, or neutral:\\\n",
        "    'The product works well most of the time, but there are moments when it suddenly stops working,\\\n",
        "    which can be frustrating. However, I think it's a decent option overall, and the design is nice,\\\n",
        "    though I expected a bit more durability for the price.'\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "374eeda3",
      "metadata": {
        "id": "374eeda3"
      },
      "source": [
        "# One shot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0923631f",
      "metadata": {
        "id": "0923631f"
      },
      "outputs": [],
      "source": [
        "# System message defines the assistant's role\n",
        "\n",
        "\n",
        "# Human message provides a one-shot example of English-to-French translation and asks for a new translation\n",
        "\n",
        "\"Translate the following sentence from English to French:\\n\"\n",
        "\"Example: 'I love data science.' => 'J'adore la science des données.'\\n\"\n",
        "\"Now translate: 'Data is the new oil.'\"\n",
        "\n",
        "\n",
        "# Send the conversation to the model\n",
        "\n",
        "\n",
        "# Output the model's response\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "968f8e17-7b59-48ec-8543-bae5a15a3899",
      "metadata": {
        "id": "968f8e17-7b59-48ec-8543-bae5a15a3899"
      },
      "source": [
        "## Few Shots\n",
        "\n",
        "One of the most effective ways to improve model performance is to give a model examples of what you want it to do. The technique of adding example inputs and expected outputs to a model prompt is known as \"few-shot prompting\". The technique is based on the Language Models are Few-Shot Learners paper. There are a few things to think about when doing few-shot prompting:\n",
        "\n",
        "How are examples generated?\n",
        "How many examples are in each prompt?\n",
        "How are examples selected at runtime?\n",
        "How are examples formatted in the prompt?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "fa21146b",
      "metadata": {
        "id": "fa21146b"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import(\n",
        "    ChatPromptTemplate,\n",
        "    FewShotChatMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "f7e466e1",
      "metadata": {
        "id": "f7e466e1"
      },
      "outputs": [],
      "source": [
        "# We can create some examples to show our AI what we want\n",
        "examples = [\n",
        "    {\"input\":\"2+2\", \"output\": \"4\"},\n",
        "    {\"input\":\"2+3\", \"output\": \"5\"},\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "e642aa2c",
      "metadata": {
        "id": "e642aa2c"
      },
      "outputs": [],
      "source": [
        "#Create an example_prompt\n",
        "example_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\"),\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7b556f2d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b556f2d",
        "outputId": "8380ef74-802d-4c9b-eb2d-c69a0b674304"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: 2+2\n",
            "AI: 4\n",
            "Human: 2+3\n",
            "AI: 5\n"
          ]
        }
      ],
      "source": [
        "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "    example_prompt = example_prompt,\n",
        "    examples=examples,\n",
        ")\n",
        "print(few_shot_prompt.format())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "21b35de6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21b35de6",
        "outputId": "36857a1e-ee5b-4e21-aa12-c2ac8f19894d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('name', None)\n",
            "('examples', [{'input': '2+2', 'output': '4'}, {'input': '2+3', 'output': '5'}])\n",
            "('example_selector', None)\n",
            "('input_variables', [])\n",
            "('optional_variables', [])\n",
            "('input_types', {})\n",
            "('output_parser', None)\n",
            "('partial_variables', {})\n",
            "('metadata', None)\n",
            "('tags', None)\n",
            "('example_prompt', ChatPromptTemplate(input_variables=['input', 'output'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=['output'], input_types={}, partial_variables={}, template='{output}'), additional_kwargs={})]))\n"
          ]
        }
      ],
      "source": [
        "for item in few_shot_prompt:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "6fb71dab",
      "metadata": {
        "id": "6fb71dab"
      },
      "outputs": [],
      "source": [
        "system_message=\"You are a wonderous wizard of math.\"\n",
        "human_template=\"{input}\"\n",
        "\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(system_message)\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "final_prompt = ChatPromptTemplate.from_messages(\n",
        "    [system_message_prompt,\n",
        "     few_shot_prompt,\n",
        "     human_message_prompt]\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d18877bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d18877bc",
        "outputId": "ab4509ee-1127-4e27-f575-eeb0d5a5f90e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='The square root of 9 is 3.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 53, 'total_tokens': 63, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-de2ee4f7-12b1-4cc5-9a93-ef4149172523-0' usage_metadata={'input_tokens': 53, 'output_tokens': 10, 'total_tokens': 63, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n"
          ]
        }
      ],
      "source": [
        "# We will use chain here each item is called a runnable,\n",
        "# we will dive deeper in the future session\n",
        "\n",
        "chain = final_prompt | gpt\n",
        "\n",
        "results=chain.invoke('What is the square root of 9?')\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06e580f1",
      "metadata": {
        "id": "06e580f1"
      },
      "outputs": [],
      "source": [
        "examples = [\n",
        "    {\n",
        "        \"question\": \"Who lived longer, Muhammad Ali or Alan Turing?\",\n",
        "        \"answer\": \"\"\"\n",
        "            Are follow up questions needed here: Yes.\n",
        "            Follow up: How old was Muhammad Ali when he died?\n",
        "            Intermediate answer: Muhammad Ali was 74 years old when he died.\n",
        "            Follow up: How old was Alan Turing when he died?\n",
        "            Intermediate answer: Alan Turing was 41 years old when he died.\n",
        "            So the final answer is: Muhammad Ali\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"When was the founder of craigslist born?\",\n",
        "        \"answer\": \"\"\"\n",
        "            Are follow up questions needed here: Yes.\n",
        "            Follow up: Who was the founder of craigslist?\n",
        "            Intermediate answer: Craigslist was founded by Craig Newmark.\n",
        "            Follow up: When was Craig Newmark born?\n",
        "            Intermediate answer: Craig Newmark was born on December 6, 1952.\n",
        "            So the final answer is: December 6, 1952\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Who was the maternal grandfather of George Washington?\",\n",
        "        \"answer\": \"\"\"\n",
        "            Are follow up questions needed here: Yes.\n",
        "            Follow up: Who was the mother of George Washington?\n",
        "            Intermediate answer: The mother of George Washington was Mary Ball Washington.\n",
        "            Follow up: Who was the father of Mary Ball Washington?\n",
        "            Intermediate answer: The father of Mary Ball Washington was Joseph Ball.\n",
        "            So the final answer is: Joseph Ball\n",
        "        \"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Are both the directors of Jaws and Casino Royale from the same country?\",\n",
        "        \"answer\": \"\"\"\n",
        "            Are follow up questions needed here: Yes.\n",
        "            Follow up: Who is the director of Jaws?\n",
        "            Intermediate Answer: The director of Jaws is Steven Spielberg.\n",
        "            Follow up: Where is Steven Spielberg from?\n",
        "            Intermediate Answer: The United States.\n",
        "            Follow up: Who is the director of Casino Royale?\n",
        "            Intermediate Answer: The director of Casino Royale is Martin Campbell.\n",
        "            Follow up: Where is Martin Campbell from?\n",
        "            Intermediate Answer: New Zealand.\n",
        "            So the final answer is: No\n",
        "        \"\"\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1b43404",
      "metadata": {
        "id": "f1b43404"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "# Define the structure for individual examples\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0f99cc7",
      "metadata": {
        "id": "f0f99cc7"
      },
      "outputs": [],
      "source": [
        "# Invoke the example prompt with the first example\n",
        "print(example_prompt.invoke(examples[0]))\n",
        "print(example_prompt.invoke(examples[0]).to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33fd84a2",
      "metadata": {
        "id": "33fd84a2"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import FewShotPromptTemplate\n",
        "\n",
        "prompt = FewShotPromptTemplate(\n",
        "\n",
        ")\n",
        "\n",
        "print(\n",
        "    prompt.invoke({\"input\": \"Who was the father of Mary Ball Washington?\"}).to_string()\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}